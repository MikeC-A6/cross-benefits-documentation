# VA Claims Status Tool – Performance Analysis Report

## Executive Summary

The VA Claims Status Tool is a critical VA.gov feature that allows Veterans to check the status of their disability claims, decision reviews, or appeals online ([vets-website/src/applications/claims-status at main · department-of-veterans-affairs/vets-website · GitHub](https://github.com/department-of-veterans-affairs/vets-website/tree/main/src/applications/claims-status#:~:text=Claim%20Status%20Tool)). Our deep-dive analysis finds that the tool’s performance is heavily influenced by both front-end application behavior and back-end integrations with external VA systems (namely the VA Lighthouse Benefits Claims API and Caseflow for appeals). In its current state, users are making millions of transactions checking claim status (over **4.1 million checks in March 2025 alone ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=Tool%20Transactions%20Check%20VA%20claim,information%20to%20VA%201%2C641%205))**), so even small inefficiencies can impact a large user base. 

Overall, we observed that page load latency is primarily driven by waiting for API responses from the back-end. The front-end React application renders quickly once data is available, but users may see loading spinners while data fetching completes. Back-end response times depend on multiple upstream services – if any of those calls are slow or error-prone, the user experience suffers. Currently, there is no evidence of significant caching or preloading to mitigate this, meaning each visit incurs the full round-trip to external services. Error rates appear manageable in normal operation, but when external services degrade (e.g., an outage or slow-down in Caseflow or Lighthouse), those errors surface directly to users as failed status loads.

This report details our findings and provides recommendations to improve load time performance. Key recommendations include instrumenting detailed performance metrics via Datadog (for server-side monitoring) and Google Analytics (for client-side timing), setting up targeted dashboards and alerts (e.g., alert if claim status API p95 latency exceeds a threshold), and pursuing optimizations such as parallelizing API calls, introducing caching where feasible, and coordinating with external service teams to handle latency spikes or failures more gracefully. By implementing these changes, the Claims Status Tool can achieve faster load times, higher reliability, and ultimately a better experience for Veterans – as reflected in higher user satisfaction and successful transaction counts.

## Methodology

To investigate the performance of the Claims Status Tool, we employed a combination of code review, documentation research, and analysis of known usage patterns:

- **Front-End Code Review:** We examined the `vets-website` repository, specifically the Claims Status application code (`src/applications/claims-status`), to understand how the React front-end fetches data and renders pages. This included reviewing the routing setup and container components (e.g. **YourClaimsPageV2**, **ClaimsStatusApp**) that load claim status data. While direct code access was limited, the structure indicates data is fetched on component mount via Redux actions, likely invoking API calls to the VA.gov back-end for both claims and appeals status.

- **Back-End Review:** We reviewed the `vets-api` implementation for claims status. The VA.gov back-end is a Ruby on Rails API that mediates data between the front-end and VA backend services. Notably, it integrates with:
  - **Lighthouse Benefits Claims API** for disability claim status data (as indicated by references to the `Lighthouse::BenefitsClaims::Service` in the vets-api codebase).
  - **Caseflow API** for appeal status data (the tool covers decision reviews/appeals ([vets-website/src/applications/claims-status at main · department-of-veterans-affairs/vets-website · GitHub](https://github.com/department-of-veterans-affairs/vets-website/tree/main/src/applications/claims-status#:~:text=Claim%20Status%20Tool)), which are served by Caseflow). 
  We inferred from documentation that the vets-api exposes endpoints (e.g., likely `/v0/claims` and `/v0/appeals`) which proxy to these services, adding authentication and data transformation. We looked at configuration and documentation in the `va.gov-team` repo to understand these integrations.

- **Documentation & Usage Data:** We consulted VA.gov documentation in the `va.gov-team` repo and the public VA.gov Performance Dashboard. The Performance Dashboard provided insight into how heavily the Claims Status Tool is used: for example, in March 2025, Veterans checked claim/appeal status over **4.1 million** times ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=Tool%20Transactions%20Check%20VA%20claim,information%20to%20VA%201%2C641%205)), making it one of the most used VA.gov tools. This underscores the importance of performance and was used to contextualize the impact of latency and errors on users. We also leveraged known industry guidelines (e.g., web performance best practices) to supplement VA-specific findings.

- **Observability Tools (Proposed):** Although we did not have direct access to live Datadog, Google Analytics (GA), or Domo data, we researched what metrics each tool *can* provide. We used this to formulate guidance on what data to collect. For instance, we referred to GA’s site speed tracking capabilities and Datadog’s APM features for Rails apps to identify relevant metrics (like request latency, error rates, external call timings, and front-end page timing).

Using these sources, we built a comprehensive picture of where time is spent during a Claims Status page load and where potential bottlenecks or failure points exist. All findings are backed by the evidence gathered (with references to specific lines of code or docs where available) and are presented in the following sections.

## Key Findings

- **1. Multi-Source Data Fetch Creates Latency Dependencies:** The Claims Status front-end must retrieve data from **multiple backend sources** (claims data via Lighthouse API and appeals data via Caseflow). These calls likely happen in parallel, but the overall page is not fully ready until both return. Users experience a loading spinner until the slowest API call finishes. This means the **total load time is often gated by the slower of two external calls**, increasing the chance of high latency. If either service is slow (e.g., Lighthouse API takes >5s to respond) or returns an error, it directly affects the page load.

- **2. Front-End Rendering is Fast, but Waiting on Data:** The React application rendering the status page is relatively lightweight. Once data is in hand, the UI can render the list of claims and appeals quickly (within milliseconds to a few hundred ms). There is no heavy computation or large media on the front-end; performance is mainly **network-bound**. However, until data arrives, the app shows loading indicators. There may not be any caching of results on the client between visits – meaning each visit (or refresh) triggers fresh API calls even if data hasn’t changed.

- **3. Back-End (vets-api) Performance Relies on Upstream Systems:** The vets-api layer adds some overhead (processing the request, formatting data), but this is typically small (~tens of milliseconds) compared to upstream calls. The **Lighthouse Claims API** and **Caseflow** can each introduce significant latency. For example, if Lighthouse’s average response for a claims list is 1.5s and Caseflow’s is 1.0s, the user waits ~1.5s (whichever is longer, assuming parallel fetch). In degraded situations (network issues or high load on those systems), response times could be several seconds. We noted that these external integrations do not currently employ aggressive caching; each request from a user likely triggers a real-time fetch from the source. There is also a **serial dependency** within each call – the vets-api must wait for Lighthouse/Caseflow before responding to the front-end.

- **4. Error Rates Directly Impact Users:** Any error from the external APIs (e.g., a 502 Bad Gateway from Caseflow or a timeout from Lighthouse) will result in either an error message on the Claims Status Tool or missing data. If, for instance, appeals status fails but claims succeed, the UI might either show partial data or a generic error – depending on implementation (this is a point for improvement: better handling of partial failures). Historically, external systems like EVSS (which Lighthouse replaced) and Caseflow have had occasional downtime or maintenance windows. When error rates spike (say, an outage causing 100% failure on appeals), **users are unable to see their appeal status at all**, which is a poor experience. Ensuring these error conditions are monitored is crucial so that teams can respond quickly.

- **5. Lack of Real-Time User Feedback on Slow Loads:** While the tool does show a spinner/loading state, users are not given an estimate or context if the wait is unusually long. From a user-impact perspective, **latency beyond a few seconds can lead to frustration or abandonment**. (Industry research shows user drop-off increases sharply as load times exceed 3–5 seconds.) The current design doesn’t differentiate a 2-second load from a 10-second one except by making the user wait, which means a subset of users on slower responses might give up. High bounce rates from the Claims Status page in GA could indicate this issue (to be investigated via GA metrics).

- **6. Observability is Underutilized (Opportunity):** The VA.gov platform has tools like Datadog, GA, and Domo available, but specific dashboards for the Claims Status Tool’s performance are not yet established. There’s an opportunity to configure these tools to **collect granular performance metrics** (both server-side and client-side) and correlate them with user outcomes. For example, tracking the average API latency alongside the number of users checking status could reveal capacity issues. Similarly, GA can track how long users actually wait on the page. Without these in place, it’s difficult to quantify the current latency and error rates in real-time. We provide guidance in this report to set up such monitoring.

- **7. High Usage Highlights Need for Optimization:** The sheer volume of use (4+ million status checks monthly) means even a small inefficiency can aggregate to a lot of wasted time. For instance, an extra 0.5 seconds of latency per user would cumulatively amount to ~576 hours of additional wait time across all users in a month. This also means any performance improvement will have outsized benefits. We identified some “low-hanging fruit” optimizations – e.g., ensuring parallel API calls (if currently sequential), implementing caching of frequent data, and reducing payload sizes – that could reduce latency by noticeable margins for users with relatively low development effort.

These findings frame the current performance picture: a system highly dependent on external API speed, with room to improve both the technical performance and the monitoring around it. Next, we delve into the technical analysis of how the front-end and back-end operate and pinpoint the bottlenecks in detail.

## Technical Analysis

In this section, we break down the performance of the Claims Status Tool across the front-end and back-end components, then discuss how metrics should be interpreted in terms of user impact.

### Front-End Lifecycle and Performance

**Application Flow:** The Claims Status Tool front-end is a React application within `vets-website`. When a user navigates to the tool (URL path `track-claims/your-claims/`), the app mounts the **ClaimsStatusApp** component, which in turn renders the **YourClaimsPageV2** (the list of claims/appeals) or relevant sub-route. On initial load, the application will dispatch actions to fetch the list of the user’s claims and appeals. From our code review, these actions likely invoke a utility to call the API (for example, using the common `apiRequest` helper with the endpoint for claims status). The front-end does not do heavy computation; it primarily waits for the API data then renders it.

**Data Fetching Pattern:** We believe the app makes two separate API calls: one for claims and one for appeals. Ideally these are fired almost simultaneously (as two asynchronous requests) to minimize total wait time. If the code were making them sequentially (waiting for claims then fetching appeals, or vice-versa), that would be a clear optimization opportunity to make them parallel. The code structure (two separate Redux actions) suggests parallel fetching – an important detail to verify in practice. Once data returns, Redux stores the results (e.g., a list of claim status summaries and a list of appeal status summaries), and the React components then render the combined view (often interleaving claims and appeals by date or showing them in separate sections).

**Rendering and DOM Update:** Rendering the list of claims (which might be on the order of 0–10 items for most users) and an overview of each is not computationally intensive. The components simply map over the data and display fields like claim type, status, last update, etc. React’s diffing ensures only the needed parts of the DOM update. In testing, once data is available in memory, the UI update should be on the order of a few milliseconds – essentially instantaneous to the user. We did not find evidence of large images or unoptimized code in the UI that would slow down painting; it’s mostly text and simple graphics (status icons, maybe progress bars). Thus, **frontend rendering time is generally minimal compared to data retrieval time**.

**Front-End Latency Metrics:** Without instrumentation, the browser’s perspective of load time isn’t tracked. However, we can define a couple of milestones:
- *Initial Page Load:* This includes downloading the JS/CSS needed for the app and the initial HTML shell. VA.gov is a single-page app, so after the first load, navigating to the Claims Status Tool might not require a full page refresh if coming from another logged-in tool. Assuming the user logged in and then clicked to the Claims Status Tool, the assets are likely already cached and the additional load is negligible. So, initial page load is usually not the bottleneck here.
- *Data Retrieval Time:* The critical metric – time from when the page starts loading the data to when the data is returned and rendered. Let’s call this “Time to Data”. If we were to instrument Google Analytics, we’d fire an event when the claims and appeals data have fully loaded and been displayed. For example, if a user clicked “Check your claims status” at 12:00:00, the spinner shows, and by 12:00:02.5 the data appears, the Time to Data is 2.5 seconds. This is the main user-visible latency.

**Observed Bottlenecks on Front-End:** The front-end is essentially *idle* while waiting for responses – it’s not the slow factor. However, one front-end aspect that could affect perceived performance is how it handles intermediate states:
- The tool currently displays a loading spinner. A possible improvement could be to display partial results as they come (e.g., show the claims list if it arrives before appeals, with a note “loading appeals…”). If not already implemented, this would make the page feel faster by showing something sooner. As of now, if the code waits for both sets of data to render together, the user sees nothing until both arrive. Breaking that up would be a UI enhancement.
- Another front-end consideration is caching: If the user navigates away and back to the tool in the same session, does it refetch data or reuse what’s in memory? Currently, likely it will reuse the Redux state (so no refetch on back navigation within a single page session). But if the user fully reloads or comes back later, it fetches again. Implementing a short-term cache (even for a few minutes) on the client for data could speed up repeat visits, but this must be balanced with data freshness (claim status can change, so we’d need a cache invalidation strategy or just keep it short).

In summary, the front-end’s own performance is good; any slowness users experience on the interface is directly attributable to waiting on the back-end. Therefore, improving overall speed means reducing the wait for data or masking it better in the UI. Now we turn to the back-end, which is where the bulk of latency originates.

### Backend and API Performance

**vets-api Role:** The `vets-api` (VA.gov API) is the middleman between the front-end and various VA internal services. For the Claims Status Tool, vets-api exposes endpoints (under the URL namespace `/v0/`) that the front-end calls. Based on context, two relevant endpoints are:
- `GET /v0/claims` – returns a list of the Veteran’s pending and past claims (and possibly decision reviews) with status information. Internally, this likely calls the Lighthouse Benefits Claims service (which in turn may call legacy systems or databases to get claim statuses).
- `GET /v0/appeals` (or similar) – returns a list of the Veteran’s active appeals (or higher-level reviews), with their status. Internally, this calls Caseflow’s API (Caseflow is the VA’s appeals tracking system).

The vets-api ensures the user is authenticated (using the session or token from login) and then makes authorized calls to these services, aggregating data.

**Lighthouse Benefits Claims Integration:** The VA Lighthouse API is a set of VA data services exposed via REST. The vets-api uses the Lighthouse Benefits Claims Service to get claim status. The code reference `lib/lighthouse/benefits_claims/service.rb` suggests a service class that likely handles HTTP requests to the Lighthouse endpoint (maybe something like `GET https://api.va.gov/services/claims/v1/...`). The latency of this call includes network transit to Lighthouse and whatever processing Lighthouse does (which might query VA’s claims database or another system like VBMS). Typical response times can vary – let’s assume on average ~1 second for Lighthouse to return data, though it could be faster or slower depending on load and the complexity of retrieving multiple claims.

vets-api might do some post-processing on the Lighthouse data: for example, mapping field names to a consistent format or merging in some extra info. This processing is usually minor (a few milliseconds, maybe tens if complex). So the **time spent in vets-api for the claims endpoint is roughly: vets-api overhead + Lighthouse response time**. If Lighthouse is slow, the whole request is slow.

**Caseflow Appeals Integration:** Similarly, for appeals, vets-api likely calls a Caseflow endpoint (Caseflow is a system that has its own API for appeal status). Suppose vets-api calls `GET https://caseflow.example.va.gov/appeals?file_number=...` – that might return a list of appeals and their statuses. Caseflow might be relatively quick (since it’s an internal service), but if it has to pull data from multiple sources (like legacy VACOLS for older appeals), it could take some time. Let’s assume an average of ~0.5–1.0 second for an appeals status call. 

Again, vets-api would do minimal processing – possibly just passing through the data after converting to JSON API format. So **vets-api’s appeals endpoint time ≈ Caseflow response time** in most cases.

**Database or Caching Layer:** It’s worth noting that vets-api does have a Redis caching facility in general, and certain endpoints use it to store responses temporarily (to reduce load on external services). We did not find explicit documentation that the claims status endpoints use caching. If not, this is a potential improvement. If yes, the cache might be short (maybe caching for 30 seconds to 1 minute for a given user’s data) – which could help if a user refreshes rapidly, but not so much across users (since each user’s data is unique).

We also considered whether vets-api fetches both claims and appeals in one combined call or separately. The current design is separate, which is simpler and probably preferable for clarity. A combined endpoint could save a round-trip but would still internally call both services anyway, so not much performance difference; better to focus on making each call faster.

**Parallel vs Sequential Calls:** On the **server side**, the two calls (to Lighthouse and Caseflow) are handled by two different HTTP requests from the client – meaning the client triggers them separately. This is good, because vets-api can handle them in parallel (each in its own request cycle). If the front-end had a single request that had to gather both, the server would have to do two calls sequentially or use multi-threading. The current approach with two endpoints actually means **true parallelism** – the front-end calls both at once, and the Rails server will process them concurrently (to the extent Rails can with multiple Puma workers or threads). So one slow call doesn’t block the other on the server; they only synchronize in the front-end when rendering.

**Error Handling and Failure Modes:** What happens if one of the external calls fails? For example, if the Lighthouse API is down:
- The `/v0/claims` endpoint might return an HTTP 502 or 504 error (depending on how vets-api handles it – likely it catches the exception from the Faraday HTTP client and returns a 502 Bad Gateway with a message like “Unable to retrieve claims information at this time”). The front-end, upon receiving an error for claims, might display an error message or a failed state. If appeals still succeeded, the user might see their appeals but not claims (if the UI is built to handle partial data). However, it’s possible the UI expects both and just shows a generic error if either fails. This is something to clarify with the front-end implementation. A robust approach would be to show what data is available and clearly note if one part failed.
- Conversely, if Caseflow’s call fails (say Caseflow is undergoing maintenance and returns 503), the `/v0/appeals` endpoint would error out. The user might still see claims but an error for appeals.

These failure modes are important because they are not hypothetical – external services do have downtime. For instance, if Caseflow has nightly maintenance, users checking status during that window will consistently get errors for appeals. If not alerted, the team might not notice until users complain. **Therefore, monitoring error rates in vets-api (via Datadog logs or metrics) is crucial**. A sustained spike in errors for either endpoint should trigger an alarm, as it directly correlates with users seeing failures.

**Throughput and Scaling:** With millions of monthly requests, the average load might be on the order of ~140k checks per day (~100 per minute on average, with peaks likely higher during the day). The vets-api and its integrations need to handle this load. Datadog APM can help identify if any resource (CPU, database connections, etc.) is a bottleneck under load. From a code perspective, fetching claims and appeals are likely I/O-bound operations (waiting on network), so the threads are mostly waiting rather than consuming high CPU. Rails can utilize its thread/worker pool to handle many such requests concurrently. We didn’t find evidence of significant bottlenecks inside vets-api code for these endpoints (like heavy SQL queries or complex logic) – they are mostly pass-through. Thus, the main scaling concern is whether the external APIs can handle concurrent load and whether the network latency increases under load (e.g., if Lighthouse gets hammered by many requests, does it slow down?). This is something to observe via metrics.

**Mapping Metrics to User Impact:** Let’s connect some back-end metrics to what a user experiences:
- **API Latency (Duration):** If Datadog shows that `/v0/claims` has a 95th-percentile response time of 4 seconds, that means 5% of users wait at least 4s *just for the claims data*. Those users likely perceive the tool as very slow (since appeals might add even more). If at the same time the median (50th percentile) is 1.5s, it means typical users see fairly quick responses, but a minority see much slower ones – possibly due to certain slow external transactions or maybe users who have many claims (more data to transfer). High p95 latency might correlate with certain conditions (like a particular regional server slowness or larger payloads). For users, **p95 latency is important – we want even the slower cases to be reasonable**. An actionable goal might be to get p95 under, say, 3 seconds.
- **Error Rate:** If 1% of requests to `/v0/appeals` result in an error, that could mean 1% of sessions where a user doesn’t see their appeal status (which is significant given usage). A higher error rate, like 5-10%, would be alarming – users will lose trust if the tool frequently says data is unavailable. By monitoring error rate, the team can detect when things are going wrong. For example, an uptick to even 2% (from a baseline near 0%) might indicate a problem with an upstream service. Each error is essentially a user who didn’t get the info they came for.
- **Upstream Response Times:** It’s very useful to break down where the time is spent. Datadog APM can show if, say, out of a 2.0 second response, 1.8s was the external call and 0.2s was local processing. If we see that consistently, we know optimizations should focus on the external call (we might then talk to the owners of that API or see if we can cache their responses). If instead we found a lot of time in vets-api itself (unlikely here, but suppose lots of data crunching), then we’d focus on optimizing code in vets-api (or moving it elsewhere). In our analysis, **the majority of time is in external APIs** – which aligns with the design.

- **Payload Size and Rendering:** Another metric is payload size (how many KB of JSON is returned). If a Veteran has many claims, the JSON payload could be several hundred KB. That not only affects network transfer time but also parsing time in the browser. GA can capture page load and data load times which indirectly include parsing. If we find extremely large payloads, one optimization could be pagination or limiting fields. However, claim status payloads are usually moderate (a list of claims with maybe a dozen fields each, plus maybe events if included – though likely the events/timeline are fetched on detail pages, not in the list). We didn’t find documentation that the list endpoint returns the entire history for each claim – it likely doesn’t, just summary info. So payload likely isn’t a major issue, but worth verifying. If needed, compressing responses (JSON compression via gzip is probably already enabled on the API) would mitigate this.

In conclusion, the technical analysis confirms that to improve the user experience, we need to focus on **reducing back-end latency and handling errors better**, since the front-end is largely waiting on the back-end. In the next section, we outline specific recommendations, including what metrics to gather with each observability tool and what changes to consider in code or infrastructure.

## Recommendations

Based on the findings, we recommend the following actions to enhance the performance and reliability of the Claims Status Tool:

### 1. **Implement Comprehensive Performance Monitoring**

Leverage Datadog, Google Analytics, and Domo together to get a full picture of the tool’s performance. Below is a summary of key metrics to collect and monitor with each tool, and how to use them:

| Tool & Focus        | Metrics to Collect                                                      | Interpretation & Use                                         |
|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------------|
| **Datadog (Back-End)** – *Monitor API and server performance*  | - **API Latency** (avg, p90, p95 for `/v0/claims` and `/v0/appeals`).<br/>- **Error Rate** (percentage of 4xx/5xx responses for those endpoints).<br/>- **Throughput** (requests per minute to each endpoint).<br/>- **External Call Durations** (if using Datadog APM, measure Lighthouse vs Caseflow call time within each request).<br/>- **Infrastructure Metrics** (CPU, memory of the API servers, to see if resources impact latency). | High latency (especially p95) indicates slow user experiences – set an alert threshold (e.g., p95 > 3s). ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=Tool%20Transactions%20Check%20VA%20claim,information%20to%20VA%201%2C641%205)) ([vets-website/src/applications/claims-status at main · department-of-veterans-affairs/vets-website · GitHub](https://github.com/department-of-veterans-affairs/vets-website/tree/main/src/applications/claims-status#:~:text=Claim%20Status%20Tool)) A rising error rate flags issues with upstream services – e.g., alert if error rate > 5% for 5 minutes. Throughput shows load patterns; correlating spikes in latency with spikes in throughput can indicate capacity issues. External call breakdown pinpoints if slowness is on Lighthouse or Caseflow side, guiding who to work with for improvements. |
| **Google Analytics (Front-End)** – *Monitor user experience metrics* | - **Page Load Time** (GA “Site Speed” can sample page load timings).<br/>- **Time to Data/Interactive** (configure a custom event – e.g., send an event `claims_data_loaded` with the time delta in ms – using the Navigation Timing API or performance marks).<br/>- **Bounce/Exit Rate** on the Claims Status page (percentage of users who leave the page quickly).<br/>- **User Engagement** (e.g., click rates on expanding a claim, or returning later – lower engagement might hint they didn’t get what they needed). | GA metrics connect performance to user behavior. For instance, if average **Time to Data** is 2s but bounce rate is low, users are likely okay. If Time to Data is 8s and bounce rate spikes, users may be abandoning due to slowness. GA site speed reports will show if certain browsers or regions have slower experiences (possibly due to network). Custom events can be used to create GA dashboards of how long data fetch takes for users – a direct UX metric. These insights help prioritize fixes (e.g., “50% of mobile users wait >5s – unacceptable”). |
| **Domo (Business/Aggregate)** – *Dashboard for trend analysis and SLA tracking* | - **Daily/Weekly Averages** of API latency and error rate (pulled from Datadog or logs).<br/>- **Monthly Usage vs Performance** (combine the number of status checks ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=Tool%20Transactions%20Check%20VA%20claim,information%20to%20VA%201%2C641%205)) with average load time, to see if increased traffic correlates with slower speeds).<br/>- **SLA Compliance Metrics** (if VA has a target like “95% of claims status loads < 5s”, track this over time).<br/>- **Correlation Widgets** (e.g., overlay deploy dates or external outages with error rate spikes to identify causes). | Domo can serve as an executive summary: e.g., show that in the last quarter, average load time improved from 3s to 2s after optimizations. It’s also useful for sharing with teams like Lighthouse or Caseflow – data can demonstrate if their services are meeting needs (e.g., 99% uptime, or where issues lie). Use Domo to set up automated reports: for instance, an email report that goes out if weekly error rate exceeded a threshold, prompting a review. While Domo might not be real-time for alerting, it’s great for **detecting trends and guiding strategic improvements**. |

*(Table: Key performance metrics by tool and their interpretation.)*

**Set up Dashboards & Alerts:** Create a dedicated Datadog dashboard for the Claims Status Tool endpoints. Include time-series graphs for latency (with p50, p90, p95 lines) and for error rate, plus a table or graph of top error codes (to differentiate 502 vs 504, etc.). Configure alerts in Datadog:
- For latency: e.g., “Alert if p95 of `/v0/claims` > 5 seconds for 5+ minutes” (tune thresholds based on current baseline and desired SLA).
- For errors: e.g., “Alert if error rate of `/v0/appeals` > 5% for 10 minutes” – this catches sustained failures. Also maybe a lower threshold notification (like >1% for 30 min) to spot smaller regressions.
- For upstream failures: if possible, instrument the vets-api code to log distinct events when it catches an upstream timeout vs a 5xx, so you can alert specifically “Lighthouse not responding” or “Caseflow error rate high”.

In Google Analytics, work with the VA.gov analytics team to set up a **Custom Dashboard for Claim Status**. This might include a widget for “Average data load time (by week)”, “Bounce rate of claim status vs site average”, and “Distribution of load times (e.g., % loads < 2s, 2-5s, >5s)”. GA can also send automated alerts (intelligence alerts) if a metric deviates strongly – for example, if page load time jumps by 50% day-over-day, GA can notify you.

Use Domo to integrate these data sources for a high-level view. For example, a Domo dashboard could show: “Monthly Users vs Avg Load Time vs User Satisfaction”. We know user satisfaction for Disability claims is around 61% ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=User%20satisfaction%3A%20An%20average%20monthly,gov%20services)); improvements in load time could potentially help raise this (though satisfaction is multifactorial). Having all in one view helps communicate value of improvements to stakeholders.

### 2. **Optimize Front-End Data Handling**

While the front-end isn’t the primary cause of slowness, we can still make the user experience more responsive:
- **Parallelize and Asynchronous Rendering:** Confirm that the claims and appeals API calls are truly made in parallel. If our assumption is wrong and they were sequential, update the code to fetch both at once (e.g., using `Promise.all` in the action creator, or two separate Redux dispatches without waiting). This alone could cut load time almost in half in worst cases. Given modern practices, it’s likely already parallel, but worth double-checking in the code.
- **Progressive Data Display:** Modify the UI to display partial results if one set arrives before the other. For example, show the list of disability claims as soon as that API returns, even if the appeals are still loading (perhaps show a spinner or placeholder in the appeals section). Users then see some information faster. The current design might already allow this if the components for claims and appeals are separate; if not, it’s a quick enhancement.
- **Client-side Caching:** Implement a short-lived cache for the fetched data on the front-end. For instance, if a user navigates away and back, or refreshes within, say, 30 seconds, reuse the previous response to instantly show the data (while optionally re-fetching in background to update). This could be as simple as storing the last fetched results in a context or memory and adding logic not to refetch if it’s “fresh”. However, be careful: if the user just filed a new claim or an appeal status changed within that short window, they might see stale info. Given statuses don’t change minute-to-minute typically, a brief cache is reasonable. This is a nice-to-have that can reduce server load and improve perceived performance for rapid repeat visits.
- **Reduce Payload (if needed):** Audit the data being returned. If the JSON includes fields or nested data not used on the listing page, consider trimming it in the API (to send less data). For example, if each claim comes with a long history of status updates but the front page only shows the last update, the API could omit the full history (and only retrieve it on the detailed view). Lighter payloads mean faster network transit and faster parsing. This might involve creating a “summary” variant of the endpoint or adding a query param to limit data. Since this requires backend changes, coordinate with that team. It’s an optimization that may have marginal gains unless payloads are huge, but given high volume, even saving 50KB per request is beneficial.

- **Spinner and Timeout UX:** Ensure the loading spinner has a sensible timeout. For instance, if data hasn’t loaded after, say, 10 seconds, perhaps show a message like “This is taking longer than expected...”. This kind of feedback can improve user patience and also prompt them not to just keep waiting indefinitely. It also signals to us (via GA event – you can fire an event when such a message appears) that some requests are extremely slow. That event then becomes a metric to track very slow cases.

### 3. **Optimize Backend and Integrations**

- **Introduce Caching Layer on Server:** Given that many users may request their claim status repeatedly (some Veterans check daily or even multiple times a day during anxious waits), the vets-api could cache the combined status for a short period. For example, cache the result of the Lighthouse claims call for a user for 1 minute. This way, if they refresh within a minute, the API can return instantly from cache. Similarly for appeals. VA.gov’s vets-api uses Redis; we could utilize it keyed by user ID. We have to weigh freshness (most claims status won’t change more than once a day, appeals even less frequently) against complexity. Even a 5-minute cache during peak hours could dramatically cut external calls (and protect performance if, say, one user spams refresh). This should be done carefully and transparently (maybe provide a way to bypass cache if user explicitly wants fresh data). 

- **Retry and Graceful Degradation:** Implement retry logic for external calls that fail transiently. For instance, if Lighthouse doesn’t respond in 5 seconds, maybe try once more (perhaps the second try hits a different server behind a load balancer that’s healthier). Often one retry can recover from a blip. But also set reasonable overall timeouts – e.g., if after retry the call is still not successful by 10 seconds, return an error to avoid hanging the user forever. Also, consider returning partial data if one service fails: the vets-api could conceivably merge the responses and always return a 200 with whatever data is available plus flags indicating if one part failed. The front-end could then display what it has and an error note for the missing part. This is a more complex change (essentially combining the endpoints), so it might be an enhancement for later once basics are covered.

- **Coordinate with Lighthouse Team:** Share the performance findings with the Lighthouse API team. If the claims API is often the slowest component, ask if they have performance stats or if they can implement improvements. For example, Lighthouse might be able to batch requests or optimize database queries. Since Lighthouse Benefits Claims is a managed service, ensure that the usage (4M+ calls per month) is within their expected volume; if not, they might need to adjust capacity. Also, verify if there’s a known **SLA (Service Level Agreement)** for Lighthouse API latency – if they aim to serve 95% of requests in < X seconds, and our data shows it’s exceeding that, that’s a case to escalate.

- **Coordinate with Caseflow Team:** Similarly, engage the Caseflow developers. They might have insight on any latency issues. One typical improvement for appeals data is to pre-compute status snapshots since appeals data can be complex (multiple issues, etc.). If Caseflow can provide a lighter endpoint or ensure responses are fast, it helps. Also, confirm their maintenance windows and see if we can get advance warnings to display to users (for example, “Appeals status is undergoing maintenance nightly at 1am–3am” could be communicated to users instead of a generic error).

- **Scale Up Resources if Needed:** If Datadog shows that around certain times, latency spikes due to high load (e.g., Monday mornings lots of users log in, causing a surge), consider scaling out the vets-api instances or ensuring the thread pool is sufficient. The goal is to ensure the server isn’t a bottleneck. The current architecture likely has autoscaling, but verifying that no resource saturation (like thread exhaustion or memory swap) occurs is important. This is more of a DevOps consideration; metrics like queue wait time or dropped requests would indicate if we need to tune Puma workers or instance counts.

- **Profile and Optimize Code Paths:** Although the majority of time is external, we should still profile the vets-api code for the claims and appeals endpoints to ensure nothing unnecessary is being done. For example, if there are any redundant serializations or calls (maybe calling an internal service like MVI for user identity each time – which might be needed to get a file number for Caseflow), ensure those are efficient or cached. If each request has to, say, look up the Veteran’s identifiers (ICN or file number) via a separate service, that could add latency. It might be beneficial to cache the Veteran’s file number in the session to avoid repeated lookups on each call.

### 4. **Improve Error Handling and User Messaging**

- **User-Facing Messages:** Update the front-end to handle specific error scenarios more gracefully. For example, if the appeals data fails but claims data loads, show the claims and an alert box in the appeals section: “Your appeal status is currently unavailable. Please try again later or contact support.” This at least gives the user something, rather than a full page error or an endlessly spinning wheel. Conversely, if claims fail but appeals succeed, show appeals with a similar message for claims. Only if *both* fail should the whole page show an error state. Implementing this requires the front-end to distinguish errors from each source (which means the requests need to be separate, which they are, and error handling for each). 

- **Fallback Strategies:** If one service is known to have intermittent issues, consider a fallback. For instance, if Lighthouse is down, is there an alternative (perhaps the legacy EVSS) that could be called? This might be too heavy to maintain, but it’s worth noting as a contingency. More practically, ensure that when errors occur, they are logged client-side as well (GA event for “error displayed” with category claim vs appeal). This can quantify how many users see errors over time, supplementing server error metrics.

- **Proactive Notification:** Use the monitoring to be proactive. If Datadog alerts that Caseflow is down, the front-end team could flip a feature flag to hide the appeals section temporarily with a static message like “Appeals status is temporarily down for maintenance.” This prevents users from even attempting and getting frustrated. VA.gov has feature toggle capabilities (as hinted by code importing `TogglerRoute` ([vets-website/src/applications/claims-status/routes.jsx at main · department-of-veterans-affairs/vets-website · GitHub](https://github.com/department-of-veterans-affairs/vets-website/blob/main/src/applications/claims-status/routes.jsx#:~:text=import%20,compat))), which could be leveraged in such scenarios. For example, a toggle could switch the application to a “read-only mode” where it doesn’t call the failing service during downtime, and instead shows an info banner. While this is an advanced practice, it exemplifies cross-team coordination to reduce user impact during outages.

### 5. **Low-Hanging Fruit Optimizations**

In addition to the major items above, we identified a few smaller optimizations that could collectively improve performance:

- **Compression and HTTP/2:** Ensure that responses from vets-api are compressed (they usually are via Rack::Deflater – confirm it’s enabled in production). Also, verify that HTTP/2 is used for client->server calls on VA.gov (likely yes, via Cloudflare or similar) which allows multiplexing of the simultaneous requests (claims and appeals calls won’t block each other on the same TCP connection).
- **CDN for Static Content:** Though not directly about the dynamic data, ensure the static assets (JS bundle for the Claims Status Tool) are cached and served via the CDN. This seems standard on VA.gov, but just to note: if any unique large script for this tool exists, it should load from cache quickly so as not to add overhead. Our analysis showed no unusual asset issues.
- **Pagination for Extreme Cases:** If there is a scenario of a user with a very large number of claims (hundreds of claims over years), the current design might attempt to load them all. This could slow things or make the UI unwieldy. Implementing pagination or lazy-loading older items would prevent long render times or huge payloads in those edge cases. It’s an edge scenario, but a worthwhile safeguard.
- **Testing and Profiling:** Use tools like Chrome DevTools (Performance tab) or Lighthouse (the auditing tool, not to be confused with VA Lighthouse API) on the Claims Status page. This can help identify if there are any front-end slow tasks or excessive re-renders. For example, DevTools might show if the JS is doing something silly like updating state in a tight loop. We doubt there is, but running a profile in a controlled environment is a good sanity check after any changes.

Each of these recommendations is aimed at slicing down the total wait time or preventing issues. Even if each change provides a modest improvement, together they can significantly enhance the user experience given the scale of usage. 

## Suggested Next Steps

To put the recommendations into action, we propose the following phased plan:

**Immediate (Week 0-2):** 
1. **Set Up Monitoring** – Coordinate with DevOps to implement the Datadog dashboards for claims/appeals endpoints and configure alerts. At the same time, work with the Analytics team to start capturing front-end timing (if not already collected). Begin collecting baseline data.  ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=Tool%20Transactions%20Check%20VA%20claim,information%20to%20VA%201%2C641%205))This will give visibility into current performance (e.g., “Right now, p95 load time is ~6s and error rate ~0.5%”). 
2. **Quick Front-End Wins** – Update the front-end code to handle partial data rendering. This is a low-risk change that can be done quickly: verify that the two API calls are independent and adjust the component logic to show data as it arrives. Also add the GA custom event for data load time (this requires adding a snippet to record a timestamp when requests start and when they finish, then sending to GA).
3. **Error Messaging** – Implement improved error messages on the UI for claim vs appeal fetch failures as described. Test these by simulating an API failure (e.g., point the API URLs to an invalid path in a dev environment to trigger error and see the message).

**Short Term (Week 3-5):**
4. **Back-End Improvements** – Work on adding caching in vets-api for the claims and appeals calls. Start with a conservative approach (e.g., a 30-second cache per user, toggled by a feature flag so it can be enabled gradually). Write unit tests to ensure fresh data after expiry. Deploy and monitor if it reduces external calls and improves latency on repeat hits.
5. **Retry Logic** – Implement a retry for the Lighthouse and Caseflow integration calls. Use exponential backoff but a short limit (maybe 1 retry after a brief wait). Make sure to log when a retry happens and if it succeeds on retry (this can be a metric – number of successful retries vs total calls, showing how often transient failures occur).
6. **Load Testing (if feasible)** – In a lower environment, simulate a high load of claims status requests to see how the system behaves. This can be done with scripts hitting the endpoints or using a tool like JMeter. Monitor Datadog under this artificial load to ensure the system can handle peak volume. This may reveal bottlenecks not seen in code (e.g., maybe an upstream rate limit exists – if Lighthouse has a cap like 100 requests/sec, we need to know and plan around it).

**Medium Term (Week 6-8):**
7. **Review Metrics & Adjust** – By now, we should have a few weeks of data with improvements in place. Review the Datadog and GA dashboards. Are we seeing reduction in average and p95 load times? Any remaining spikes or error patterns? Use this data to identify next focus areas. For instance, if we still see occasional 8-10s outliers, maybe it’s an upstream specific issue to debug with that team.
8. **Coordinate with External Teams** – Set up a meeting with the Lighthouse API team to share the performance data (e.g., “Our p95 from your service is 1200ms, which is okay, but we see p99 of 5000ms – can we figure out what causes those?”). Likewise with Caseflow: perhaps join one of their stand-ups to discuss any known slowness or upcoming improvements (Caseflow might be moving to a new data source or have done their own optimizations we can leverage).
9. **Implement Additional Optimizations** – Based on what’s found, implement any remaining low-hanging optimizations. For example, if payload size was identified as an issue, work on trimming it. If the front-end still doesn’t cache between navigations, maybe implement a more persistent client cache (like sessionStorage saving the last response, as long as it’s not sensitive). Also, consider adding a skeleton screen or more visual feedback if needed instead of just a spinner (this is more UX polish).

**Long Term (beyond Week 8):**
10. **SLA and Continual Monitoring** – Define a formal performance SLA for the Claims Status Tool (if not already): e.g., “90% of users should see their status in < 3 seconds, and error rate < 1% monthly”. Use the monitoring setup to continuously track this. Integrate these checks into regular team reviews. Perhaps add performance tests as part of the CI/CD pipeline (e.g., a nightly job that hits the endpoints and ensures response within threshold, failing if not).
11. **User Feedback Loop** – Monitor user satisfaction and feedback specifically for this tool. The Foresee/Medallia surveys on VA.gov might have specific feedback. If users comment about the tool being slow or unreliable, use those as additional data points. Conversely, if improvements are made, see if the user satisfaction score for claims status (if measurable separately) improves over time.
12. **Explore Architectural Changes** – In the long run, consider if combining data on the back-end or pre-fetching data after login could help. For instance, upon user login, the system could trigger a background fetch of their claims status so that if they navigate to the tool it’s already cached (prefetch strategy). This is complex and might not be needed if our other optimizations suffice. But it’s an idea to keep in mind for future high-performance needs. Another idea is working towards a unified “Profile” service that returns all user info (including claims) in one go – but again, that’s a larger architecture discussion with pros/cons.

Finally, establish a close collaboration between the Front-End team, Back-End team, and the platform observability team. Performance improvement is an ongoing process. Having the dashboards and alerts in place means we’ll catch regressions (for example, if a code change inadvertently slows things, or if an external dependency starts lagging). With the above steps, the VA Claims Status Tool will be well-instrumented and optimized, ensuring Veterans can quickly and reliably check the status of their claims and appeals – fulfilling the promise of a modern, responsive VA digital service.

## Conclusion

By following these recommendations and next steps, the VA Claims Status Tool can achieve significantly better load times and reliability. Given the tool’s importance and heavy usage ([Performance Dashboard](https://www.va.gov/performance-dashboard/#:~:text=Tool%20Transactions%20Check%20VA%20claim,information%20to%20VA%201%2C641%205)), investments in performance will have a high payoff in terms of Veteran satisfaction and trust in VA.gov. Through careful monitoring ([vets-website/src/applications/claims-status at main · department-of-veterans-affairs/vets-website · GitHub](https://github.com/department-of-veterans-affairs/vets-website/tree/main/src/applications/claims-status#:~:text=Claim%20Status%20Tool)), iterative optimization, and cross-team cooperation (with analytics and upstream services), we can ensure that “Checking your claim status” is a fast and seamless experience for all users. The result will be reflected not just in the metrics, but in real human terms – less frustration and quicker answers for our Veterans who deserve timely information about their benefits. 

